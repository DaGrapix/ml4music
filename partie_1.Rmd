
```{r Librairie,include=FALSE, results='hide'}
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)

rm(list=ls())
```
## Analyse descriptive

On commence par importer les données et regarder de manière générale de quoi est composé notre jeu de donnée.

```{r import,include=TRUE}
setwd(getwd())
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)

dim(data)
n <- nrow(data)
p <- ncol(data)
```

Les dimensions du dataset importé sont correctes. Il y a bien 192 variables pour 4278 vecteurs de données.


```{r, R.options=list(max.print=5)}
summary(data)
```


```{r}
## A FAIRE : Analyse uni-bi variée

##Question : Comment choisir les variables qu'on observe ? 
```

```{r frequences,include=TRUE}
# Proportion des genres musicaux
freq<-plyr::count(data,'GENRE')

freq

prop_classical<-freq[1,2]/n
prop_jazz<-freq[2,2]/n

prop_classical
prop_jazz
```




```{r Distribution et transformation log,echo=FALSE}

density_plot = function(X,xlab,lxlab){
  density<-ggplot(data,aes(x=X))+geom_density(col="blue")+xlab(xlab)
  log_density<-ggplot(data,aes(x=log(X)))+geom_density(col="red")+xlab(lxlab)
  plot_grid(density,log_density,labels=c("Densité","Densité log"),label_size=12,ncol=1,label_x = 0, label_y = 0,hjust = -0.5, vjust = -0.5)
}

density_plot(data$PAR_SC,xlab="PAR_SC",lxlab="log(PAR_SC)")
density_plot(data$PAR_SC_V,xlab="PAR_SC_V",lxlab="log(PAR_SC_V)")
density_plot(data$PAR_ASC_V,xlab="PAR_ASC_V",lxlab="log(PAR_ASC_V)")


```



On remarque que les variables PAR_SC_V et PAR_ASC_V ont des distributions qui ne sont pas gaussiennes avant la transformation log au contraire des autres variables comme PAR_SC. C'est pourquoi, on leur applique une transformation log.

Par ailleurs, il est indiqué dans la description du jeu de donnée, que les variables 148 à 167 sont les mêmes que celles de 128 à 147. Ainsi, on peut s'en séparer sans risquer de perdre des informations sur notre jeu de donnée.

```{r transformation doublons}
rm(list=ls())
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)

data.old<-data

data <- data.old[,-c(128:147)]
data$PAR_SC_V<-log(data$PAR_SC_V)
data$PAR_ASC_V<-log(data$PAR_ASC_V)

n <- nrow(data)
p <- ncol(data)
```


### Variables très corrélées
```{r}
corr <- cor(x=data[,-p])

#selection des indices de la matrice de correlation > 0.99
high.corr.index.new <- which(corr > 0.99, arr.ind = TRUE) %>% unname

#selection des indices appartenant a la matrice triangulaire inferieure stricte,
#pour retirer les doublons, ainsi que les elements diagonaux.
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
high.corr.index.new
```
```{r variables corrélées}
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]], 
                         names(data)[high.corr.index.new[,2]]), nrow=nrow(high.corr.index.new))
correlated.variables
```

```{r indices dans le df original, include=FALSE}
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data) %in% name.list), nrow=nrow(high.corr.index.new))
high.corr.index
```

On remarque que les deux premiers couples de variables très corrélées sont en fait
les deux dernières mesures associées respectivement aux variables PAR_ASE et PAR_ASEV.

Le dernier couple de corrélation très élevée montre que la variable *PAR_ZCD* est très corrélée
avec *PAR_ZCD_10FR_MEAN* qui semble être une moyenne de *PAR_ZCD*.

On veillera à bien retirer à chaque fois l'une des deux variables très corrélées,
en effet les garder augmenterait la dimmension et la complexité du modèle, sans pour autant
apporter de l'information utile.

On retirera par exemple les variables *PAR_ASE34*, *PAR_ASEV34* et *PAR_ZCD_10FR_MEAN*

```{r}
data <- data[,-high.corr.index.new[,1]]
n <- nrow(data)
p <- ncol(data)

dim(data)
```


### Cas des variables *PAR_ASE_M*, *PAR_ASE_MV*, *PAR_SFM_M* et *PAR_SFM_MV*

Dans le but de continuer notre analyse et de pouvoir utiliser la fonction glm par la suite , il nous faut créer une variable catégorielle y qui prenne comme valeur uniquement 0 et 1.
Par exemple, le Jazz vaut 0 et le classique vaut 1.

```{r Création de la variable y,include=TRUE}

y<-ifelse(data$GENRE=="Classical",1,0)

## On ajoute cette colonne au dataframe

ndata<-cbind(data,y)

## Représentation des valeurs d'une variable selon son genre musical, plot classique et boxplot.

myplot = function(x,Y,xlab=""){
  plot(x,Y,xlab=xlab, col=Y+1,pch=Y+1);
  boxplot(x~Y,xlab=xlab,horizontal=TRUE)
}

myplot(ndata$PAR_ASE_M,ndata$y,"PAR_ASE_M")    
myplot(ndata$PAR_ASE_MV,ndata$y,"PAR_ASE_MV")
myplot(ndata$PAR_SFM_M,ndata$y,"PAR_SFM_M")
myplot(ndata$PAR_SFM_MV,ndata$y,"PAR_SFM_MV")

```

Je pense que ce qu'il faut voir c'est que des fortes valeurs de PAR_ASE_M par exemple, la musique a plus de chance d'être Classique alors que pour des faibles valeurs de PAR_ASE_M, elle a plus de chance d'être du Jazz.
De même, pour des grandes valeurs de PAR_ASE_MV,PAR_SFM_M, la musique aura plus de chance d'être du Jazz.

Ce sont des valeurs qui jouent un rôle important dans la détermination de notre genre musical, ce sont des features importantes pour un modèle.


```{r}
library(corrplot)
var.data <- data[ , names(data) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV")]
var.corr <- cor(var.data)
corrplot(var.corr)


```


## Echantillon d'apprentissage
```{r}
set.seed(103)
train=sample(c(TRUE,FALSE),n,rep=TRUE,prob=c(2/3,1/3))
```

## Estimation de modèle

On commence par créer nos différents modèles à l'aide de la fonction glm du package stats.



```{r Définition des modèles,include=TRUE}

## Définition de Mod0

Mod0<-glm(y~PAR_TC+PAR_SC+PAR_SC_V+PAR_ASE_M,PAR_ASE_MV+PAR_SFM_M+PAR_SFM_MV,family=binomial,data=ndata)
summary(Mod0)

## Pour ModT, il faut enlever les variables fortement corrélées et peut être aussi les autres qui sont évoquées.

## Étape 1 : modifier ndata en fonction des variables que l'on enlève

## Étape 2 : Définir le modèle :


## Définition de ModT
ModT<-glm(y~.,family=binomial,data=ndata[,-p])
summary(ModT)

## On récupère les p-value des variables

p_value<-coef(summary(ModT))[-1,4]
## On sélectionne celle qui ont un niveau de significativité de 5% et on crée la formule de notre modèle Mod1

index_var_Mod1<-which(p_value>0.05)
var_Mod1<-names(data[index_var_Mod1])
formula_Mod1<-as.formula(paste("y ~",paste(var_Mod1, collapse= "+")))


Mod1<-glm(formula = formula_Mod1,family=binomial,data=ndata[-p])


index_var_Mod2<-which(p_value>0.2)
var_Mod2<-names(data[index_var_Mod2])
formula_Mod2<-as.formula(paste("y ~",paste(var_Mod2, collapse= "+")))

Mod2<-glm(formula = formula_Mod2,family=binomial,data=ndata[-p])


## ModAIC
library(MASS)
#step <- stepAIC(ModT)

```
```{r}
#step$anova
```


## Courbes ROC
```{r Import des données de test}
test_data<-read.csv("Music_test.txt",header=TRUE,sep=";")
dim(test_data)
```
On remarque que les jeux de test à les mêmes défauts que le jeu d'apprentissage, on va donc lui faire subir les mêmes transformations.

```{r Transformation jeu d'apprentissage}
test_data <- test_data[,-c(128:147)]
test_data$PAR_SC_V<-log(test_data$PAR_SC_V)
test_data$PAR_ASC_V<-log(test_data$PAR_ASC_V)

```
```{r Courbe ROC,include=TRUE}

predproba = predict(ModT,type="response") # ModT est le résultat de glm
pred = prediction(predproba,ndata$y) # objet S4
plot(performance(pred,"sens","fpr"),xlab="",col=2) # ROC
legend("bottomright",legend="Apprentissage ModT",col="red")
```

## Erreurs