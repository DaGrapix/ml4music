
```{r Librairie,include=FALSE, results='hide'}
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)

rm(list=ls())
```
## Analyse descriptive

On commence par importer les données et regarder de manière générale de quoi est composé notre jeu de donnée.

```{r import,include=TRUE}
setwd(getwd())
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)

dim(data)
n <- nrow(data)
p <- ncol(data)
```

Les dimensions du dataset importé sont correctes. Il y a bien 192 variables pour 4278 vecteurs de données.


```{r, R.options=list(max.print=5)}
summary(data)
```


```{r}
## A FAIRE : Analyse uni-bi variée

##Question : Comment choisir les variables qu'on observe ? 
```

```{r frequences,include=TRUE}
# Proportion des genres musicaux
freq<-plyr::count(data,'GENRE')

freq

prop_classical<-freq[1,2]/n
prop_jazz<-freq[2,2]/n

prop_classical
prop_jazz
```


Il est indiqué dans la description du jeu de donnée, que les variables 148 à 167 sont les mêmes que celles de 128 à 147. Ainsi, on peut s'en séparer sans risquer de perdre de l'information sur notre jeu de donnée. C'est d'ailleurs d'autant plus intéressant de les retirer, puisque cela réduit la dimmension et la complexité du modèle.



```{r transformation doublons}
data <- data[,-c(128:147)]
n <- nrow(data)
p <- ncol(data)
dim(data)
```
Il reste bien 172 variables pour 4278 vecteurs de données.


Par ailleurs, on remarque que les variables PAR_SC_V et PAR_ASC_V ont des distributions qui ne sont pas gaussiennes avant la transformation log au contraire des autres variables comme PAR_SC.

```{r Distribution et transformation log,echo=TRUE}
density_plot = function(X,xlab,lxlab){
  density<-ggplot(data,aes(x=X))+geom_density(col="blue")+xlab(xlab)
  log_density<-ggplot(data,aes(x=log(X)))+geom_density(col="red")+xlab(lxlab)
  plot_grid(density,log_density,labels=c("Densité","Densité log"),label_size=12,ncol=1,label_x = 0, label_y = 0,hjust = -0.5, vjust = -0.5)
}

density_plot(data$PAR_SC,xlab="PAR_SC",lxlab="log(PAR_SC)")
density_plot(data$PAR_SC_V,xlab="PAR_SC_V",lxlab="log(PAR_SC_V)")
density_plot(data$PAR_ASC_V,xlab="PAR_ASC_V",lxlab="log(PAR_ASC_V)")
```

On leur applique donc une transformation log.

```{r transformation doublons}
data$PAR_SC_V <- log(data$PAR_SC_V)
data$PAR_ASC_V <- log(data$PAR_ASC_V)

density_plot(data$PAR_SC,xlab="PAR_SC",lxlab="log(PAR_SC)")
density_plot(data$PAR_SC_V,xlab="PAR_SC_V",lxlab="log(PAR_SC_V)")
density_plot(data$PAR_ASC_V,xlab="PAR_ASC_V",lxlab="log(PAR_ASC_V)")
```


### Variables très corrélées
```{r}
corr <- cor(x=data[,-p])

#selection des indices de la matrice de correlation > 0.99
high.corr.index.new <- which(corr > 0.99, arr.ind = TRUE) %>% unname

#selection des indices appartenant a la matrice triangulaire inferieure stricte,
#pour retirer les doublons, ainsi que les elements diagonaux.
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
high.corr.index.new
```

Nom des variables corrélées:
```{r variables corrélées}
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]], 
                         names(data)[high.corr.index.new[,2]]), nrow=nrow(high.corr.index.new))
correlated.variables
```

Indices des variables corrélées dans le dataframe original:
```{r indices dans le df original, include=FALSE}
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data) %in% name.list), nrow=nrow(high.corr.index.new))
high.corr.index
```

On remarque que les deux premiers couples de variables très corrélées sont en fait
les deux dernières mesures associées respectivement aux variables PAR_ASE et PAR_ASEV.

Le dernier couple de corrélation très élevée montre que la variable *PAR_ZCD* est très corrélée
avec *PAR_ZCD_10FR_MEAN* dont le nom semble qu'il s'agit d'une moyenne des *PAR_ZCD*.

On veillera à bien retirer à chaque fois l'une des deux variables très corrélées,
en effet les garder augmenterait la dimmension et la complexité du modèle, sans pour autant
apporter de l'information utile.

On retirera par exemple les variables *PAR_ASE34*, *PAR_ASEV34* et *PAR_ZCD_10FR_MEAN*.

```{r}
data <- data[,-high.corr.index.new[,1]]
n <- nrow(data)
p <- ncol(data)

dim(data)
```
Il reste bien 169 variables pour 4278 vecteurs de données.












### Cas des variables *PAR_ASE_M*, *PAR_ASE_MV*, *PAR_SFM_M* et *PAR_SFM_MV*

La description du jeu de données indique que les variables *PAR_ASE_M*, *PAR_ASE_MV*, *PAR_SFM_M* et *PAR_SFM_MV* ne sont en fait que des combinaisons linéaires et moyennes des autres variables. Ce ne sont pas des variables intrinsèques au jeu de données. On veillera, comme précédemment, à les retirer du jeu de données.

```{r}
#indices <- which(colnames(data) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))

#data <- data[, -indices]
#n <- nrow(data)
#p <- ncol(data)
#dim(data)
```
(Il reste bien 165 variables pour 4278 vecteurs de données.)










## Echantillon d'apprentissage

Dans le but de continuer notre analyse et de pouvoir utiliser la fonction glm par la suite, il nous faut créer une variable catégorielle y qui prenne comme valeur uniquement 0 et 1 selon le genre de l'échantillon.
On choisira donc arbitrairement le Jazz vaut 1 et le classique vaut 0.

```{r}
set.seed(103)
train=sample(c(TRUE,FALSE),n,rep=TRUE,prob=c(2/3,1/3))

x <- data[, -p]
y <- ifelse(data$GENRE=="Jazz", 1, 0)

train = sample(c(TRUE,FALSE), n, rep=TRUE, prob=c(2/3,1/3))
x.train <- x[which(train),]
x.test <- x[which(train==FALSE),]

y.train <- y[which(train)]
y.test <- y[which(train==FALSE)]

data.train <- cbind(x.train, y=y.train)
data.test <- cbind(x.test, y=y.test)

n.train <- nrow(data.train)
p.train <- ncol(data.train)

n.test <- nrow(data.test)
p.test <- ncol(data.test)

dim(data.train)
dim(data.test)
```

## Estimation de modèle

On commence par créer nos différents modèles à l'aide de la fonction glm du package stats.


```{r Définition des modèles,include=TRUE}
## Définition de Mod0
Mod0 <- glm(y~PAR_TC+PAR_SC+PAR_SC_V+PAR_ASE_M,PAR_ASE_MV+PAR_SFM_M+PAR_SFM_MV, family=binomial, data=data.train)
summary(Mod0)

## Définition de ModT
ModT <- glm(y~., family=binomial, data=data.train)
summary(ModT)

## On récupère les p-value des variables
p_value <- coef(summary(ModT))[-1,4]

## On sélectionne celles qui ont un niveau de significativité de 5% et on crée la formule de notre modèle Mod1
index.var.Mod1 <- which(p_value>0.05)
var.Mod1 <- names(data[index.var.Mod1])
formula.Mod1 <- as.formula(paste("y ~",paste(var.Mod1, collapse= "+")))

Mod1<-glm(formula <- formula.Mod1, family=binomial, data=data.train)

## On sélectionne celles qui ont un niveau de significativité de 20% et on crée la formule de notre modèle Mod2
index.var.Mod2 <- which(p_value>0.2)
var.Mod2 <- names(data[index.var.Mod2])
formula.Mod2 <- as.formula(paste("y ~",paste(var.Mod2, collapse= "+")))

Mod2 <- glm(formula=formula.Mod2, family=binomial, data=data.train)
```


```{r stepAIC,include=TRUE}
library(MASS)

## Attention execution longue

#step <- stepAIC(ModT)
#step$anova
```

Modèle final:
```{r}
## Selection des variables eliminees
removed.variables <- gsub('- ', '', step$anova$Step[-1])
variable.names <- names(data)[-p]

## Selection des variables à garder
keep.indices <- ifelse(variables %in% removed.variables, FALSE, TRUE) %>% which()
variables.AIC <- variable.names[keep.indices]

## Creation de la formule
formula.AIC <- as.formula(paste("y ~",paste(variables.AIC, collapse= "+")))
formula.AIC
```

```{r}
## Modèle final
ModAIC <- glm(formula=formula.AIC, family=binomial, data=data.train)
```



## Courbes ROC
```{r Import des données de test}

```
On remarque que les jeux de test à les mêmes défauts que le jeu d'apprentissage, on va donc lui faire subir les mêmes transformations.

```{r Transformation jeu d'apprentissage}
test_data <- test_data[,-c(128:147)]
test_data$PAR_SC_V <- log(test_data$PAR_SC_V)
test_data$PAR_ASC_V <- log(test_data$PAR_ASC_V)

```
```{r Courbe ROC,include=TRUE}

predproba = predict(ModT,type="response") # ModT est le résultat de glm
pred = prediction(predproba,ndata$y) # objet S4
plot(performance(pred,"sens","fpr"),xlab="",col=2) # ROC
legend("bottomright",legend="Apprentissage ModT",col="red")
```

## Erreurs