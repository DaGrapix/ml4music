knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(caret)
install.packages("tibble")
install.packages("tibble")
library(caret)
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
####################################
############# Partie 3 #############
####################################
set.seed(564)
#Mini projet
#Anthony Kalaydjian - Mathieu Occhipinti
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
require(caret)
?trainControl
?caret::trainControl
knn_ctrl <- caret::trainControl(method="cv",number=10)
library(caret)
knn_ctrl <- trainControl(method="cv",number=10)
library(caret)
####################################
############# Partie 3 #############
####################################
set.seed(564)
library(caret)
install.packages(tibble)
install.packages("tibble")
install.packages("tibble")
library(caret)
install.packages("tibble")
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
####################################
############# Partie 2 #############
####################################
rm(list=ls())
setwd(getwd())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
setwd("F:/ZZ/sta203_projet/final")
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
set.seed(314)
grid <- 10^seq(0, -4, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam <- cv.out$lambda.min
plot(cv.out)
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
# Meme après avoir retiré les variables de la partie 1,
# 108 couples de variables ont un coefficient de correlation > 75%
# 21  couples de variables ont un coefficient de correlation > 90%
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
