#calcul long
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, family = "binomial", lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
##modele final
ridge.model.0 <- glmnet(x=x.train.0, y=y.train.0, family="binomial", lambda=bestlam.0)
## Erreur d'apprentissage
probabilities.train.0 <- ridge.model.0 %>% predict(newx=x.train.0)
predicted.classes.train.0 <- ifelse(probabilities.train.0 > 0.5, 1, 0)
err.train.ridge.0 <- mean(predicted.classes.train.0 != y.train.0)
err.train.ridge.0
## Erreur de généralisation
probabilities.test.0 <- ridge.model.0 %>% predict(newx=x.test.0)
predicted.classes.test.0 <- ifelse(probabilities.test.0 > 0.5, 1, 0)
err.test.ridge.0 <- mean(predicted.classes.test.0 != y.test.0)
err.test.ridge.0
####################################
############# Partie 2 #############
####################################
rm(list=ls())
setwd(getwd())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
# Meme après avoir retiré les variables de la partie 1,
# 108 couples de variables ont un coefficient de correlation > 75%
# 21  couples de variables ont un coefficient de correlation > 90%
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
#On remarque que 2 voire 3 coefficients ont un comportement different des autres et explosent
## determination des coefficients qui explosent
coef.ridge = coef(ridge.fit)[-1,] # enlève le coefficient d'intercept qui n'apporte rien
attained.max <- apply(coef.ridge, MARGIN=1, FUN=function(x) max(abs(x)))
max.theta.values <- round(attained.max, digits=2)
#coefficient maximal
max.coefs <- c(which.max(max.theta.values))
#deuxième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
#troisième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
names(max.coefs) <- names(data)[max.coefs]
max.coefs
#Les variables associées à ces trois coefficients sont :
# PAR_SFMV24 la 126ème variable du dataset.
# PAR_SFMV2 la 104ème variable du dataset.
# PAR_THR_3RMS_10FR_VAR la 176ème variable du dataset.
## Plot de l'evolution des coefficients, en ayant retire ceux qui explosent
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
## Plot de l'evolution des coefficients en fonction du coefficient de penalite
plot(ridge.fit, xvar = "lambda")
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(0, -5, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
set.seed(314)
grid <- 10^seq(0, -4, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam <- cv.out$lambda.min
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(0, -5, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
grid <- 10^seq(0, -4, length=100)
set.seed(314)
grid <- 10^seq(0, -4, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam <- cv.out$lambda.min
plot(cv.out)
bestlam
##modele final
ridge.model <- glmnet(x=x.train, y=y.train, family="binomial", lambda=bestlam)
## Erreur d'apprentissage
probabilities.train <- ridge.model %>% predict(newx=x.train)
predicted.classes.train <- ifelse(probabilities.train > 0.5, 1, 0)
err.train.ridge <- mean(predicted.classes.train != y.train)
err.train.ridge
## Erreur de généralisation
probabilities.test <- ridge.model %>% predict(newx=x.test)
predicted.classes.test <- ifelse(probabilities.test > 0.5, 1, 0)
err.test.ridge <- mean(predicted.classes.test != y.test)
err.test.ridge
set.seed(314)
grid <- 10^seq(0, -4, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
rm(list=ls())
setwd(getwd())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
############################    Q1
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
# Meme après avoir retiré les variables de la partie 1,
# 108 couples de variables ont un coefficient de correlation > 75%
# 21  couples de variables ont un coefficient de correlation > 90%
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
#On remarque que 2 voire 3 coefficients ont un comportement different des autres et explosent
## determination des coefficients qui explosent
coef.ridge = coef(ridge.fit)[-1,] # enlève le coefficient d'intercept qui n'apporte rien
attained.max <- apply(coef.ridge, MARGIN=1, FUN=function(x) max(abs(x)))
max.theta.values <- round(attained.max, digits=2)
#coefficient maximal
max.coefs <- c(which.max(max.theta.values))
#deuxième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
#troisième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
names(max.coefs) <- names(data)[max.coefs]
max.coefs
#Les variables associées à ces trois coefficients sont :
# PAR_SFMV24 la 126ème variable du dataset.
# PAR_SFMV2 la 104ème variable du dataset.
# PAR_THR_3RMS_10FR_VAR la 176ème variable du dataset.
## Plot de l'evolution des coefficients, en ayant retire ceux qui explosent
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
## Plot de l'evolution des coefficients en fonction du coefficient de penalite
plot(ridge.fit, xvar = "lambda")
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
#On voit que la SCR minimum est atteinte pour lambda=0.01 sur la frontière du domaine.
#Il serait judicieux de ré-effectuer la validation-croisée pour des nouvelles valeurs de lambda.
#Le minimum étant sûrement atteint avant 10^0, on choisira une grille de 10^0 à 10^-5$.
set.seed(314)
grid <- 10^seq(0, -4, length=100)
#execution un peu longue
cv.out <- cv.glmnet(x=x.train, y=y.train, family="binomial", lambda=grid, nfolds=10)
bestlam <- cv.out$lambda.min
plot(cv.out)
bestlam
#on trouve 0.0008497534 qui n'est pas sur la frontière, c'est le lambda optimal.
##modele final
ridge.model <- glmnet(x=x.train, y=y.train, family="binomial", lambda=bestlam)
## Erreur d'apprentissage
probabilities.train <- ridge.model %>% predict(newx=x.train)
predicted.classes.train <- ifelse(probabilities.train > 0.5, 1, 0)
err.train.ridge <- mean(predicted.classes.train != y.train)
err.train.ridge
#0.08570425
## Erreur de généralisation
probabilities.test <- ridge.model %>% predict(newx=x.test)
predicted.classes.test <- ifelse(probabilities.test > 0.5, 1, 0)
err.test.ridge <- mean(predicted.classes.test != y.test)
err.test.ridge
#0.09713487
#On considere toutes les variables
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)]
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)]
set.seed(4658)
grid <- 10^seq(10, -2, length=100)
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, family = "binomial", lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
#nouvelle grid
set.seed(4658)
grid <- 10^seq(0, -4, length=100)
#calcul long
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, family = "binomial", lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
##modele final
ridge.model.0 <- glmnet(x=x.train.0, y=y.train.0, family="binomial", lambda=bestlam.0)
## Erreur d'apprentissage
probabilities.train.0 <- ridge.model.0 %>% predict(newx=x.train.0)
predicted.classes.train.0 <- ifelse(probabilities.train.0 > 0.5, 1, 0)
err.train.ridge.0 <- mean(predicted.classes.train.0 != y.train.0)
err.train.ridge.0
## Erreur de généralisation
probabilities.test.0 <- ridge.model.0 %>% predict(newx=x.test.0)
predicted.classes.test.0 <- ifelse(probabilities.test.0 > 0.5, 1, 0)
err.test.ridge.0 <- mean(predicted.classes.test.0 != y.test.0)
err.test.ridge.0
rm(list=objects())
setwd(getwd())
library(glmnet)
library(tidyverse)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
prepare.data.unlabelled <- function(filename){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.unlabelled <- read.csv(filename,sep=";",header=TRUE)
data.unlabelled$PAR_SC_V <- log(data.unlabelled$PAR_SC_V)
data.unlabelled$PAR_ASC_V <- log(data.unlabelled$PAR_ASC_V)
##indices à retirer
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
#On retire les variables
data.unlabelled <- data.unlabelled[, -indices.retires]
return(data.unlabelled)
}
df <- prepare.data()
#On entraîne le modèle sur tout les individus du dataset
data.train <- df$data
data.unlabelled <- prepare.data.unlabelled("Music_test.txt")
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.unlabelled <- data.unlabelled %>% as.matrix()
#Modèle de régression ridge
set.seed(314)
grid <- 10^seq(0, -4, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family = "binomial", lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
bestlam
#model final
ridge.model <- glmnet(x=x.train, y=y.train, family="binomial", lambda=bestlam)
## Prédiction
probabilities.pred <- ridge.model %>% predict(newx=x.unlabelled)
predicted.classes.pred <- ifelse(probabilities.pred > 0.5, "Jazz", "Classical")
prediction <- c(predicted.classes.pred) %>% unname()
write.table(prediction, file="KALAYDJIAN-OCCHIPINTI_yes.txt", sep=": ", row.names=FALSE, col.names=FALSE, quote=FALSE)
one <- read.table("KALAYDJIAN-OCCHIPINTI_yes.txt")
two <- read.table("KALAYDJIAN-OCCHIPINTI_test.txt")
one
mean(one != two)
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=as.factor(y.train), method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
#Erreurs
knn.pred.train.1 <- predict(knn.1, newdata = x.unlabelled)
err.train.knn.1 <- mean(knn.pred.train.1 != one)
knn.pred.train.1
err.train.knn.1 <- mean(knn.pred.train.1 != ifelse(one=="Jazz", 1, 0))
err.train.knn.1
rm(list=objects())
setwd(getwd())
library(glmnet)
library(tidyverse)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
prepare.data.unlabelled <- function(filename){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.unlabelled <- read.csv(filename,sep=";",header=TRUE)
data.unlabelled$PAR_SC_V <- log(data.unlabelled$PAR_SC_V)
data.unlabelled$PAR_ASC_V <- log(data.unlabelled$PAR_ASC_V)
##indices à retirer
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
#On retire les variables
data.unlabelled <- data.unlabelled[, -indices.retires]
return(data.unlabelled)
}
df <- prepare.data()
#On entraîne le modèle sur tout les individus du dataset
data.train <- df$data
data.unlabelled <- prepare.data.unlabelled("Music_test.txt")
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.unlabelled <- data.unlabelled %>% as.matrix()
#Modèle de régression ridge
set.seed(314)
grid <- 10^seq(0, -4, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, family = "binomial", lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
bestlam
#on trouve 0.0005857021 qui n'est pas sur la frontière, c'est le lambda optimal.
#model final
ridge.model <- glmnet(x=x.train, y=y.train, family="binomial", lambda=bestlam)
## Prédiction
probabilities.pred <- ridge.model %>% predict(newx=x.unlabelled)
predicted.classes.pred <- ifelse(probabilities.pred > 0.5, "Jazz", "Classical")
prediction <- c(predicted.classes.pred) %>% unname()
write.table(prediction, file="KALAYDJIAN-OCCHIPINTI_test.txt", sep=": ", row.names=FALSE, col.names=FALSE, quote=FALSE)
