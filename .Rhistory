knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(caret)
install.packages("tibble")
install.packages("tibble")
library(caret)
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
install.packages(c("cachem", "class", "cli", "dplyr", "evaluate", "ggplot2", "htmltools", "KernSmooth", "MASS", "nnet", "rlang", "sass", "tibble", "tseries", "viridisLite", "vroom", "xfun"))
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
####################################
############# Partie 3 #############
####################################
set.seed(564)
#Mini projet
#Anthony Kalaydjian - Mathieu Occhipinti
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
require(caret)
?trainControl
?caret::trainControl
knn_ctrl <- caret::trainControl(method="cv",number=10)
library(caret)
knn_ctrl <- trainControl(method="cv",number=10)
library(caret)
####################################
############# Partie 3 #############
####################################
set.seed(564)
library(caret)
install.packages(tibble)
install.packages("tibble")
install.packages("tibble")
library(caret)
install.packages("tibble")
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
install.packages(c("class", "cli", "dplyr", "MASS", "rlang", "tibble"))
#Mini projet
#Anthony Kalaydjian - Mathieu Occhipinti
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
setwd(getwd())
rm(list=ls())
graphics.off()
####################################
############# Partie 1 #############
####################################
############################    Q1
## importation des données
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
setwd("F:/ZZ/sta203_projet")
load("step.RData")
indices.retires
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
print(indices.retires)
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
rm(list=ls())
df <- prepare.data()
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
print(indices.retires)
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
rm(list=ls())
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
## determination des coefficients qui explosent
coef.ridge = coef(ridge.fit)[-1,] # enlève le coefficient d'intercept qui n'apporte rien
attained.max <- apply(coef.ridge, MARGIN=1, FUN=function(x) max(abs(x)))
max.theta.values <- round(attained.max, digits=2)
#coefficient maximal
max.coefs <- c(which.max(max.theta.values))
#deuxième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
#troisième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
names(max.coefs) <- names(data)[max.coefs]
max.coefs
## Plot de l'evolution des coefficients, en ayant retire ceux qui explosent
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
## Plot de l'evolution des coefficients en fonction du coefficient de penalite
plot(ridge.fit, xvar = "lambda")
## Plot de l'evolution des coefficients, en ayant retire ceux qui explosent
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
## Plot de l'evolution des coefficients en fonction du coefficient de penalite
plot(ridge.fit, xvar = "lambda")
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
############################    Q3
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
## Erreur d'apprentissage
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
err.train.ridge <- mean((ridge.pred.train - y.train)^2)
err.train.ridge
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
err.test.ridge <- mean((ridge.pred.test - y.test)^2)
err.test.ridge
#On considere toutes les variables
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)]
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)]
set.seed(4658)
grid <- 10^seq(10, -2, length=100)
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
#nouvelle grid
set.seed(4658)
grid <- 10^seq(0, -5, length=100)
##Attention calcul long
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
## Erreur d'apprentissage
ridge.pred.train.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.train.0)
err.train.ridge.0 <- mean((ridge.pred.train.0 - y.train.0)^2)
err.train.ridge.0
## Erreur de généralisation
ridge.pred.test.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.test.0)
err.test.ridge.0 <- mean((ridge.pred.test.0 - y.test.0)^2)
err.test.ridge.0
rm(list=ls())
df <- prepare.data()
rm(list=objects())
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
rm(list=objects())
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)] %>% as.factor()
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
#Erreurs
knn.pred.train.1 <- predict(knn.1, newdata = x.train)
err.train.knn.1 <- mean(knn.pred.train.1 != y.train)
err.train.knn.1
knn.pred.test.1 <- predict(knn.1, newdata = x.test)
err.test.knn.1 <- mean(knn.pred.test.1 != y.test)
err.test.knn.1
set.seed(103)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
#Erreurs
knn.pred.train.1 <- predict(knn.1, newdata = x.train)
err.train.knn.1 <- mean(knn.pred.train.1 != y.train)
err.train.knn.1
knn.pred.test.1 <- predict(knn.1, newdata = x.test)
err.test.knn.1 <- mean(knn.pred.test.1 != y.test)
err.test.knn.1
## Validation croisée
set.seed(103)
set.seed(12)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
#Erreurs
knn.pred.train.1 <- predict(knn.1, newdata = x.train)
err.train.knn.1 <- mean(knn.pred.train.1 != y.train)
err.train.knn.1
knn.pred.test.1 <- predict(knn.1, newdata = x.test)
err.test.knn.1 <- mean(knn.pred.test.1 != y.test)
err.test.knn.1
## Validation croisée
set.seed(556)
k.grid <- expand.grid(k=1:30)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#choix du modèle optimal
knn <- train(x=x.train, y=as.factor(y.train), method="knn", trControl=knn.ctrl, tuneGrid=k.grid, preProcess=c("center", "scale"))
stopCluster(cl)
print(knn)
plot(knn)
#Erreurs
knn.pred.train <- predict(knn, newdata = x.train)
err.train.knn <- mean(knn.pred.train != y.train)
err.train.knn
knn.pred.test <- predict(knn,newdata = x.test)
err.test.knn <- mean(knn.pred.test != y.test)
err.test.knn
