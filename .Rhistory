data.train <- df$data.train
data.test <- df$data.test
### Dataset reduit
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)] %>% as.factor()
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)] %>% as.factor()
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
### Dataset reduit
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)] %>% as.factor()
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
knn.pred.train.1 <- predict(knn.1, newdata = x.train)
err.train.knn.1 <- mean(knn.pred.train.1 != y.train)
err.train.knn.1
knn.pred.test.1 <- predict(knn.1, newdata = x.test)
err.test.knn.1 <- mean(knn.pred.test.1 != y.test)
err.test.knn.1
set.seed(556)
k.grid <- expand.grid(k=1:30)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#choix du modèle optimal
knn.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.grid, preProcess=c("center", "scale"))
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)] %>% as.factor()
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1.0)
#Erreurs
knn.pred.train.1.0 <- predict(knn.1.0, newdata = x.train.0)
err.train.knn.1.0 <- mean(knn.pred.train.1.0 != y.train.0)
err.train.knn.1.0
#0
knn.pred.test.1.0 <- predict(knn.1.0, newdata = x.test.0)
err.test.knn.1.0 <- mean(knn.pred.test.1.0 != y.test.0)
err.test.knn.1.0
#0.04961565
## Validation croisée
set.seed(556)
k.grid <- expand.grid(k=1:30)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#choix du modèle optimal
knn.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.grid, preProcess=c("center", "scale"))
stopCluster(cl)
print(knn.0)
#meilleur modèle obtenu pour k=1
plot(knn.0)
knn.pred.train.0 <- predict(knn.0, newdata = x.train.0)
err.train.knn.0 <- mean(knn.pred.train.0 != y.train.0)
err.train.knn.0
#0
knn.pred.test.0 <- predict(knn.0,newdata = x.test.0)
err.test.knn.0 <- mean(knn.pred.test.0 != y.test.0)
err.test.knn.0
#0.04961565
rm(list=ls())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
############################    Q1
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
# Meme après avoir retiré les variables de la partie 1,
# 109 couples de variables ont un coefficient de correlation > 75%
# 21  couples de variables ont un coefficient de correlation > 90%
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
#On remarque que 2 voire 3 coefficients ont un comportement different des autres et explosent
## determination des coefficients qui explosent
coef.ridge = coef(ridge.fit)[-1,] # enlève le coefficient d'intercept qui n'apporte rien
attained.max <- apply(coef.ridge, MARGIN=1, FUN=function(x) max(abs(x)))
max.theta.values <- round(attained.max, digits=2)
max.coefs <- c(which.max(max.theta.values))
#deuxième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
#troisième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
names(max.coefs) <- names(data)[max.coefs]
max.coefs
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
plot(ridge.fit, xvar = "lambda")
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
plot(ridge.fit, xvar = "lambda")
plot(ridge.fit)
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
plot(ridge.fit)
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
plot(ridge.fit)
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
max.coefs
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(10, -2, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
err.train.ridge <- mean((ridge.pred.train - y.train)^2)
err.train.ridge
#0.08827494
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
err.test.ridge <- mean((ridge.pred.test - y.test)^2)
err.test.ridge
#0.09808341
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)]
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)]
set.seed(4658)
grid <- 10^seq(10, -2, length=100)
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
set.seed(4658)
grid <- 10^seq(0, -5, length=100)
##Attention calcul long
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
ridge.pred.train.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.train.0)
err.train.ridge.0 <- mean((ridge.pred.train.0 - y.train.0)^2)
err.train.ridge.0
#0.0875637
ridge.pred.test.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.test.0)
err.test.ridge.0 <- mean((ridge.pred.test.0 - y.test.0)^2)
err.test.ridge.0
#0.0976252
rm(list=objects())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)] %>% as.factor()
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1 <- train(x=x.train, y=y.train, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1)
knn.pred.train.1 <- predict(knn.1, newdata = x.train)
err.train.knn.1 <- mean(knn.pred.train.1 != y.train)
err.train.knn.1
#0
knn.pred.test.1 <- predict(knn.1, newdata = x.test)
err.test.knn.1 <- mean(knn.pred.test.1 != y.test)
err.test.knn.1
#0.05520615
set.seed(556)
k.grid <- expand.grid(k=1:30)
#Calculs paralleles
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#choix du modèle optimal
knn <- train(x=x.train, y=as.factor(y.train), method="knn", trControl=knn.ctrl, tuneGrid=k.grid, preProcess=c("center", "scale"))
stopCluster(cl)
print(knn)
plot(knn)
knn.pred.train <- predict(knn, newdata = x.train)
err.train.knn <- mean(knn.pred.train != y.train)
err.train.knn
#0
knn.pred.test <- predict(knn,newdata = x.test)
err.test.knn <- mean(knn.pred.test != y.test)
err.test.knn
#0.05520615
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)] %>% as.factor()
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)] %>% as.factor()
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
k.1 <- expand.grid(k=1)
knn.1.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
k.1 <- expand.grid(k=1)
knn.1.0 <- train(x=x.train.0, y=as.factor(y.train.0), method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
stopCluster(cl)
unregister_dopar <- function() {
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
unregister_dopar()
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)] %>% as.factor()
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)] %>% as.factor()
knn.ctrl <- trainControl(method="cv", number=10)
## modele k=1
k.1 <- expand.grid(k=1)
knn.1.0 <- train(x=x.train.0, y=as.factor(y.train.0), method="knn", trControl=knn.ctrl, tuneGrid=k.1, preProcess=c("center", "scale"))
print(knn.1.0)
knn.pred.train.1.0 <- predict(knn.1.0, newdata = x.train.0)
err.train.knn.1.0 <- mean(knn.pred.train.1.0 != y.train.0)
err.train.knn.1.0
#0
knn.pred.test.1.0 <- predict(knn.1.0, newdata = x.test.0)
err.test.knn.1.0 <- mean(knn.pred.test.1.0 != y.test.0)
err.test.knn.1.0
#0.04961565
set.seed(556)
k.grid <- expand.grid(k=1:30)
#Calculs paralleles
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#choix du modèle optimal
knn.0 <- train(x=x.train.0, y=y.train.0, method="knn", trControl=knn.ctrl, tuneGrid=k.grid, preProcess=c("center", "scale"))
stopCluster(cl)
print(knn.0)
plot(knn.0)
knn.pred.test.0 <- predict(knn.0,newdata = x.test.0)
err.test.knn.0 <- mean(knn.pred.test.0 != y.test.0)
err.test.knn.0
#0.04961565
rm(list=ls())
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
library(caret)
library(class)
library(doParallel)
#Importation et nettoyage des donnees
prepare.data <- function(){
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
indices.retires <- c(148:167)
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
indices.retires <- c(indices.retires, high.corr.index[,1])
indices.4 <- which(names(data.0) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
indices.retires <- c(indices.retires, indices.4)
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
return(list(data=data, data.train=data.train, data.test=data.test, data.0=data.0, data.train.0=data.train.0, data.test.0=data.test.0))
}
df <- prepare.data()
data <- df$data
data.train <- df$data.train
data.test <- df$data.test
data.0 <- df$data.0
data.train.0 <- df$data.train.0
data.test.0 <- df$data.test.0
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
# Meme après avoir retiré les variables de la partie 1,
# 109 couples de variables ont un coefficient de correlation > 75%
# 21  couples de variables ont un coefficient de correlation > 90%
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit, xvar = "lambda")
coef.ridge = coef(ridge.fit)[-1,] # enlève le coefficient d'intercept qui n'apporte rien
attained.max <- apply(coef.ridge, MARGIN=1, FUN=function(x) max(abs(x)))
max.theta.values <- round(attained.max, digits=2)
#coefficient maximal
max.coefs <- c(which.max(max.theta.values))
#deuxième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
#troisième coefficient maximal
new.coef <- which(names(data) == names(which.max(max.theta.values[-max.coefs])))
max.coefs <- c(max.coefs, new.coef)
names(max.coefs) <- names(data)[max.coefs]
max.coefs
#Les variables associées à ces trois coefficients sont :
# PAR_SFMV24 la 126ème variable du dataset.
# PAR_SFMV2 la 104ème variable du dataset.
# PAR_THR_3RMS_10FR_VAR la 176ème variable du dataset.
## Plot de l'evolution des coefficients, en ayant retire ceux qui explosent
matplot(apply(abs(coef.ridge[-max.coefs,]) ,2 ,sum), t(coef.ridge[-max.coefs,]),
col=1:10, lty=1:10, type="l", xlab="norme L1", ylab="coefficients")
