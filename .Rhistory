knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
load("step.RData")
setwd("F:/ZZ/sta203_projet")
setwd("F:/ZZ/sta203_projet")*
load("step.RData")
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
#Mini projet
#Anthony Kalaydjian - Mathieu Occhipinti
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
setwd(getwd())
rm(list=ls())
graphics.off()
####################################
############# Partie 1 #############
####################################
############################    Q1
## importation des données
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
dim(data)
#Les données ont bien été importées
#Remarque: on garde une copie du dataframe original
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
## Analyse uni et bi variée
## Proportion des genres musicaux
proportion.classique <- mean(ifelse(data$GENRE=="Classical", 1, 0))
proportion.jazz <- mean(ifelse(data$GENRE=="Jazz", 1, 0))
proportion.classique
proportion.jazz
#Les deux catégories sont relativement équivalentes en taille, c'est bien pour la classification.
#On sauvegarde les indices variables à retirer
#Ici on retirera les variables en double
indices.retires <- c(148:167)
## Analyse de densité des variables PAR_SC, PAR_SCV et PAR_ASC_V
density_plot <- function(X,xlab,lxlab){
density <- ggplot(data.0,aes(x=X)) + geom_density(col="blue") + xlab(xlab)
log_density <- ggplot(data.0, aes(x=log(X))) + geom_density(col="red") + xlab(lxlab)
plot_grid(density, log_density, labels=c("Densité","Densité log"), label_size=12, ncol=1, label_x=0, label_y=0, hjust=-0.5, vjust=-0.5)
}
density_plot(data.0$PAR_SC, xlab="PAR_SC", lxlab="log(PAR_SC)")
density_plot(data.0$PAR_SC_V, xlab="PAR_SC_V", lxlab="log(PAR_SC_V)")
density_plot(data.0$PAR_ASC_V, xlab="PAR_ASC_V", lxlab="log(PAR_ASC_V)")
## Transformation log
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
density_plot(data.0$PAR_SC, xlab="PAR_SC", lxlab="log(PAR_SC)")
density_plot(data.0$PAR_SC_V, xlab="PAR_SC_V", lxlab="log(PAR_SC_V)")
## Variables très corrélées
#Remarque: on a ici déjà retiré les doublons
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
#selection des indices de la matrice de correlation > 0.99
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
#selection des indices appartenant a la matrice triangulaire inferieure stricte,
#pour retirer les doublons, ainsi que les elements diagonaux.
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
high.corr.index.new
#nom des couples de variables très corrélées
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
correlated.variables
#Calcul des indices de ces variables dans le dataframe d'origine
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
high.corr.index
#indices des variables corrélées
indices.corr <- c(high.corr.index[,1], high.corr.index[,2])
indices.corr
#plot des variables corrélées
corrplot(cor(data.0[,indices.corr]))
library(ggplot2)
library(plyr)
library(stats)
library(tidyverse)
library(cowplot)
library(ROCR)
library(MASS)
library(glmnet)
library(corrplot)
setwd(getwd())
rm(list=ls())
graphics.off()
####################################
############# Partie 1 #############
####################################
############################    Q1
## importation des données
data <- read.csv("Music_2023.txt",sep=";",header=TRUE)
dim(data)
#Les données ont bien été importées
#Remarque: on garde une copie du dataframe original
data.0 <- read.csv("Music_2023.txt",sep=";",header=TRUE)
## Analyse uni et bi variée
## Proportion des genres musicaux
proportion.classique <- mean(ifelse(data$GENRE=="Classical", 1, 0))
proportion.jazz <- mean(ifelse(data$GENRE=="Jazz", 1, 0))
proportion.classique
proportion.jazz
#Les deux catégories sont relativement équivalentes en taille, c'est bien pour la classification.
#On sauvegarde les indices variables à retirer
#Ici on retirera les variables en double
indices.retires <- c(148:167)
## Analyse de densité des variables PAR_SC, PAR_SCV et PAR_ASC_V
density_plot <- function(X,xlab,lxlab){
density <- ggplot(data.0,aes(x=X)) + geom_density(col="blue") + xlab(xlab)
log_density <- ggplot(data.0, aes(x=log(X))) + geom_density(col="red") + xlab(lxlab)
plot_grid(density, log_density, labels=c("Densité","Densité log"), label_size=12, ncol=1, label_x=0, label_y=0, hjust=-0.5, vjust=-0.5)
}
density_plot(data.0$PAR_SC, xlab="PAR_SC", lxlab="log(PAR_SC)")
density_plot(data.0$PAR_SC_V, xlab="PAR_SC_V", lxlab="log(PAR_SC_V)")
density_plot(data.0$PAR_ASC_V, xlab="PAR_ASC_V", lxlab="log(PAR_ASC_V)")
## Transformation log
data.0$PAR_SC_V <- log(data.0$PAR_SC_V)
data.0$PAR_ASC_V <- log(data.0$PAR_ASC_V)
density_plot(data.0$PAR_SC, xlab="PAR_SC", lxlab="log(PAR_SC)")
density_plot(data.0$PAR_SC_V, xlab="PAR_SC_V", lxlab="log(PAR_SC_V)")
## Variables très corrélées
#Remarque: on a ici déjà retiré les doublons
data <- data.0[, -indices.retires]
corr <- cor(x=data[, -ncol(data)])
#selection des indices de la matrice de correlation > 0.99
high.corr.index.new <- which(corr>0.99, arr.ind = TRUE) %>% unname
#selection des indices appartenant a la matrice triangulaire inferieure stricte,
#pour retirer les doublons, ainsi que les elements diagonaux.
lower.tri <- lower.tri(corr, diag=FALSE)
high.corr.index.new <- high.corr.index.new[which(lower.tri[high.corr.index.new]==TRUE),]
high.corr.index.new
#nom des couples de variables très corrélées
correlated.variables <- matrix(c(names(data)[high.corr.index.new[,1]],
names(data)[high.corr.index.new[,2]]),
nrow=nrow(high.corr.index.new))
correlated.variables
#Calcul des indices de ces variables dans le dataframe d'origine
name.list <- as.vector(correlated.variables)
high.corr.index <- matrix(which(names(data.0) %in% name.list), nrow=nrow(high.corr.index.new))
high.corr.index
#indices des variables corrélées
indices.corr <- c(high.corr.index[,1], high.corr.index[,2])
indices.corr
#plot des variables corrélées
corrplot(cor(data.0[,indices.corr]))
#On retirera high.corr.index[,1]
indices.retires <- c(indices.retires, high.corr.index[,1])
## Cas des variables PAR_ASE_M, PAR_ASE_MV, PAR_SFM_M et PAR_SFM_MV
#Remarque, on réutilise le dataframe original ici
indices.ASE <- c(4:37)
indices.ASEV <- c(39:72)
indices.SFM <- c(78:101)
indices.SFMV <- c(103:126)
par(mfrow=c(2,2))
data.mean.ASE <- apply(data.0[,indices.ASE], MARGIN=1, FUN=mean)
plot(x=data.mean.ASE, y=data.0$PAR_ASE_M, xlab="mean(PAR_ASE)", ylab="PAR_ASE_M")
data.mean.ASEV <- apply(data.0[,indices.ASEV], MARGIN=1, FUN=mean)
plot(x=data.mean.ASEV, y=data.0$PAR_ASE_MV, xlab="mean(PAR_ASEV)", ylab="PAR_ASE_MV")
data.mean.SFM <-apply(data.0[,indices.SFM], MARGIN=1, FUN=mean)
plot(x=data.mean.SFM, y=data.0$PAR_SFM_M, xlab="mean(PAR_SFM)", ylab="PAR_SFM_M")
data.mean.SFMV <-apply(data.0[,indices.SFMV], MARGIN=1, FUN=mean)
plot(x=data.mean.SFMV, y=data.0$PAR_SFM_MV, xlab="mean(PAR_SFMV)", ylab="PAR_SFM_MV")
#On les 4 variables étudiées sont bien les moyennes des différentes observations
#auxquelles elles sont associées, elles n'apportent pas d'information, on les retire.
#Indice des variables dans le dataframe original.
indices.4 <- which(names(data) %in% c("PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV"))
#On obtient ainsi la liste finale de toutes les variables que l'on va retirer du dataframe.
indices.retires <- c(indices.retires, indices.4)
indices.retires
length(indices.retires)
#On retirera donc 27 variables au total.
############################    Q2
#Renommage de GENRE, et variable booléenne
names(data.0)[ncol(data.0)] = "y"
data.0$y <- ifelse(data.0$y=="Jazz", 1, 0)
## Echantillon d'apprentissage
set.seed(103)
train = sample(c(TRUE,FALSE), nrow(data.0), rep=TRUE, prob=c(2/3,1/3))
#On retirera les variables plus tard
data.train.0 <- data.0[which(train),]
data.test.0 <- data.0[which(train==FALSE),]
#On retire les 27 variables
data <- data.0[, -indices.retires]
data.train <- data.train.0[, -indices.retires]
data.test <- data.test.0[, -indices.retires]
dim(data)
dim(data.train)
dim(data.test)
############################    Q3
## Estimation de modèle
#Définition de Mod0
Mod0 <- glm(y~PAR_TC+PAR_SC+PAR_SC_V+PAR_ASE_M,PAR_ASE_MV+PAR_SFM_M+PAR_SFM_MV, family=binomial, data=data.train.0)
summary(Mod0)
#Définition de ModT
ModT <- glm(y~., family=binomial, data=data.train)
summary(ModT)
#On récupère les p-value des tests de significativité des coefficients de ModT
p_value <- coef(summary(ModT))[-1,4]
#On sélectionne les variables dont le coefficient a un niveau de significativité de 5% et on crée la formule de notre modèle Mod1
index.var.Mod1 <- which(p_value>0.05)
var.Mod1 <- names(data[index.var.Mod1])
formula.Mod1 <- as.formula(paste("y ~", paste(var.Mod1, collapse="+")))
Mod1<-glm(formula <- formula.Mod1, family=binomial, data=data.train)
summary(Mod1)
#On sélectionne les variables dont le coefficient a un niveau de significativité de 20% et on crée la formule de notre modèle Mod1
index.var.Mod2 <- which(p_value>0.2)
var.Mod2 <- names(data[index.var.Mod2])
formula.Mod2 <- as.formula(paste("y ~", paste(var.Mod2, collapse="+")))
Mod2 <- glm(formula=formula.Mod2, family=binomial, data=data.train)
summary(Mod2)
## Méthode de selection de variable AIC
#stepAIC
## Attention, execution longue...
#step <- stepAIC(ModT)
#Selection des variables eliminees
removed.variables <- gsub('- ', '', step$anova$Step[-1])
corr <- cor(x=data[,-ncol(data)])
#selection des indices de la matrice de correlation > threshold
threshold <- c(0.75, 0.9)
high.corr.index <- sapply(threshold, FUN=function(x) (nrow(which(corr > x, arr.ind = TRUE)) - (ncol(data)-1))/2)
high.corr.index
x.train <- data.train[,-ncol(data.train)] %>% as.matrix()
y.train <- data.train[,ncol(data.train)]
x.test <- data.test[,-ncol(data.test)] %>% as.matrix()
y.test <- data.test[,ncol(data.test)]
dim(x.train)
############################    Q2
par(mfrow=c(1,1))
grid <- 10^seq(10, -2, length=100)
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=grid, family="binomial")
ridge.fit$dim
plot(ridge.fit)
plot(ridge.fit$lambda)
plot(ridge.fit, xvar = "lambda")
## Erreur de généralisation
ridge.pred.test = predict(ridge.fit, s=bestlam, newx=x.test)
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=bestlam, family="binomial")
## Erreur d'apprentissage
ridge.pred.train = predict(ridge.fit, s=bestlam, newx=x.train)
## Erreur d'apprentissage
ridge.pred.train = predict(ridge.fit, alpha=0, s=bestlam, newx=x.train)
mean((ridge.pred.train - y.train)^2)
err_train_ridge<-round(mean(ridge.pred.train!=y.train),3)
err_train_ridge
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
## Erreur d'apprentissage
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
mean((ridge.pred.train - y.train)^2)
err_train_ridge<-round(mean(ridge.pred.train!=y.train),3)
err_train_ridge
err_train_ridge<-round(mean(ridge.pred.train!=y.train),3)
err_train_ridge<-round(mean(ridge.pred.train!=y.train),3)
err_train_ridge
## Erreur d'apprentissage
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
err.train.ridge <- mean((ridge.pred.train - y.train)^2)
err.train.ridge
set.seed(314)
grid <- 10^seq(0, -5, length=100)
cv.out <- cv.glmnet(x=x.train, y=y.train, lambda=grid, nfolds=10)
bestlam=cv.out$lambda.min
plot(cv.out)
bestlam
ridge.fit <- glmnet(x=x.train, y=y.train, alpha=0, lambda=bestlam, family="binomial")
## Erreur d'apprentissage
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
err.train.ridge <- mean((ridge.pred.train - y.train)^2)
err.train.ridge
## Erreur de généralisation
ridge.pred.test = predict(ridge.fit, s=bestlam, newx=x.test)
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
mean((ridge.pred.test - y.test)^2)
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
err.test.ridge <- mean((ridge.pred.test - y.test)^2)
err.test.ridge
#On considere toutes les variables
x.train.0 <- data.train.0[,-ncol(data.train.0)] %>% as.matrix()
y.train.0 <- data.train.0[,ncol(data.train.0)]
x.test.0 <- data.test.0[,-ncol(data.test.0)] %>% as.matrix()
y.test.0 <- data.test.0[,ncol(data.test.0)]
set.seed(4658)
grid <- 10^seq(10, -2, length=100)
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
#nouvelle grid
set.seed(4658)
grid <- 10^seq(0, -5, length=100)
##Attention calcul long
cv.out.0 <- cv.glmnet(x=x.train.0, y=y.train.0, lambda=grid, nfolds=10)
bestlam.0 <- cv.out.0$lambda.min
plot(cv.out.0)
bestlam.0
## Erreur d'apprentissage
ridge.pred.train.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.train.0)
err.train.ridge.0 <- mean((ridge.pred.train.0 - y.train.0)^2)
err.train.ridge.0
## Erreur de généralisation
ridge.pred.test.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.test.0)
err.test.ridge.0 <- mean((ridge.pred.test.0 - y.test.0)^2)
err.test.ridge.0
err.train.ridge <- round(mean((ridge.pred.train - y.train)^2), 3)
err.train.ridge
## Erreur d'apprentissage
ridge.pred.train = predict(cv.out, alpha=0, s=bestlam, newx=x.train)
err.train.ridge <- round(mean((ridge.pred.train - y.train)^2), 3)
err.train.ridge
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
err.test.ridge <- mean((ridge.pred.test - y.test)^2)
err.test.ridge
err.test.ridge <- mean((ridge.pred.test - y.test)^2) %>% round(,3)
err.test.ridge
## Erreur de généralisation
ridge.pred.test = predict(cv.out, alpha=0, s=bestlam, newx=x.test)
err.test.ridge <- round(mean((ridge.pred.test - y.test)^2), 3)
err.test.ridge
## Erreur d'apprentissage
ridge.pred.train.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.train.0)
err.train.ridge.0 <- round(mean((ridge.pred.train.0 - y.train.0)^2), 3)
err.train.ridge.0
## Erreur de généralisation
ridge.pred.test.0 = predict(cv.out.0, alpha=0, s=bestlam.0, newx=x.test.0)
err.test.ridge.0 <- round(mean((ridge.pred.test.0 - y.test.0)^2), 3)
err.test.ridge.0
